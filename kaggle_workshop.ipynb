{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Workshop\n",
    "\n",
    "\n",
    "This dataset has attributes about police stops in Rhode Island. \n",
    "\n",
    "Can we predict the traffic stop outcome based on information about the stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the police_project.csv with read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas, read the csv \"police_project.csv\"\n",
    "policedata = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the loaded data with info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv file loaded, we must select the features we want to examine and clean up the data.\n",
    "\n",
    "We can select the columns with policedata[[\"feature1\",\"feature2\",...,\"feature6\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = policedata[[\"driver_gender\",\"driver_age\",\"driver_race\",\"violation\", \"search_conducted\", \"stop_outcome\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are any null values with isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the incomplete entries with dropna() and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.?\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the top 5 entries with head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.?()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What possible outcomes are there? Check the unique values in the stop_outcome column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our dataframe into x and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 0:5]\n",
    "y = df.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pd.get_dummies(x) to transform x into numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.?(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 80% training and 20% testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ? import ?\n",
    "# perform the split\n",
    "x_train, x_test, y_train, y_test = ?(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building SK-Learn Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a random forest model\n",
    "\n",
    "<img src=\"resources/randomforest.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the RandomForestClassifier\n",
    "from ? import ?\n",
    "\n",
    "rf_clf = ? # initialize RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a support vector machine model\n",
    "\n",
    "<img src=\"resources/svm.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC model\n",
    "from ? import ?\n",
    "\n",
    "# make several svc versions with \"rbf\" kernel\n",
    "sv_rbf_clf = SVC(kernel='rbf', gamma='scale', verbose=1)\n",
    "# make other svc versions using different kernels: \"poly\", \"sigmoid\"\n",
    "sv_poly_clf = ? # change kernel to \"poly\"\n",
    "sv_sig_clf = ? # change kernel to \"sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a K Nearest Neighbors Model\n",
    "<img src=\"resources/knn.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KNeighbors\n",
    "from ? import ?\n",
    "\n",
    "knn_clf = ? # initializae KNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to validate the model is through the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# fit\n",
    "rf_clf.?(x_train, y_train)\n",
    "# predict\n",
    "rf_pred = rf_clf.?(x_test)\n",
    "print(\"RANDOM FOREST ACC:\", metrics.accuracy_score(y_test, rf_pred))\n",
    "print(\" RANDOM FOREST SUMMARY:\", metrics.classification_report(y_test, rf_pred))\n",
    "\n",
    "sv_rbf_clf.?(x_train, y_train)\n",
    "sv_rbf_pred = sv_rbf_clf.?(x_test) #Predict class labels for samples in X_test.\n",
    "print(\"SVM RBF ACC:\", metrics.accuracy_score(y_test, sv_rbf_pred))\n",
    "print(\" SVM RBF SUMMARY:\", metrics.classification_report(y_test, sv_rbf_pred))\n",
    "\n",
    "# sv_poly_clf.?(x_train, y_train)\n",
    "# sv_poly_pred = sv_rbf_clf.?(x_test) #Predict class labels for samples in X_test.\n",
    "# print(\"SVM POLY ACC:\", metrics.accuracy_score(y_test, sv_poly_pred))\n",
    "# print(\" SVM POLY SUMMARY:\", metrics.classification_report(y_test, sv_poly_pred))\n",
    "\n",
    "# sv_sig_clf.?(x_train, y_train)\n",
    "# sv_sig_pred = sv_rbf_clf.?(x_test) #Predict class labels for samples in X_test.\n",
    "# print(\"SVM SIGMOID ACC:\", metrics.accuracy_score(y_test, sv_sig_pred))\n",
    "# print(\" SVM SIGMOID SUMMARY:\", metrics.classification_report(y_test, sv_sig_pred))\n",
    "\n",
    "knn_clf.?(x_train, y_train)\n",
    "knn_pred = knn_clf.?(x_test) #Predict class labels for samples in X_test.\n",
    "print(\"KNN ACC:\", metrics.accuracy_score(y_test, knn_pred))\n",
    "print(\" KNN SUMMARY:\", metrics.classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is cross fold validation. Either cross_val_score or cross_val_predict.\n",
    "Cross validation is useful for choosing a model and its hyperparameters.\n",
    "\n",
    "<img src=\"resources/crossval.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ? import ?\n",
    "scores = cross_val_score(?YOUR_MODEL?, x_test, y_test, cv=5, verbose=2)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data\n",
    "\n",
    "Let's say we have an imbalanced data set with 90% of one class.\n",
    "Our model can classify everything as that class and achieve 90% accuracy.\n",
    "\n",
    "Is this happening in our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we fix this?\n",
    "\n",
    "We can balance the class weights in the model.\n",
    "\n",
    "Binary Solutions:\n",
    "\n",
    "Increase minority examples through resample with replacement.\n",
    "\n",
    "Decrease majority examples through resample without replacement.\n",
    "\n",
    "Examine ROC curve instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0, class_weight = \"balanced\")\n",
    "rf_clf.fit(x_train, y_train)\n",
    "rf_pred = rf_clf.predict(x_test)\n",
    "print(metrics.accuracy_score(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to Load Data\n",
    "\n",
    "`Dataset` class must override 3 functions:\n",
    "```\n",
    "__init__()\n",
    "__len__()\n",
    "__getitem__(index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class PoliceDataset(Dataset):\n",
    "    def __init__(self, csv_file, label_column=\"stop_outcome\"):\n",
    "        # read the csv_file\n",
    "        policedata = ?\n",
    "        # select appropriate columns\n",
    "        df = policedata[[\"driver_gender\",\"driver_age\",\"driver_race\",\"violation\", \"search_conducted\", \"stop_outcome\"]]\n",
    "        # remove NAs\n",
    "        df = df.?()\n",
    "        \n",
    "        # Convert to category names into numbers\n",
    "        # save a mapping of integers to label strings\n",
    "        self.label_map = dict(enumerate(df['stop_outcome'].astype(\"category\").cat.categories))\n",
    "        cat_columns = df.select_dtypes(['object']).columns\n",
    "        for c in cat_columns:\n",
    "            df[c] = df[c].astype(\"category\").cat.codes        \n",
    "\n",
    "        self.features = ?.values\n",
    "        self.labels = ?.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return ? # [DEFINE LENGTH]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        # return the features and the labels\n",
    "        return (\n",
    "            ?, # return features\n",
    "            ? # return labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a PoliceDataset\n",
    "my_dataset = ?\n",
    "features, labels = my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap it in a DataLoader\n",
    "from ? import ?\n",
    "my_dataloader = ?(?, batch_size=16)\n",
    "my_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(my_dataloader))\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our label mapping\n",
    "[my_dataset.label_map[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Model\n",
    "\n",
    "`nn.Module` class must override 3 functions:\n",
    "```\n",
    "__init__()\n",
    "forward()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers=3,num_classes= 6, width=1000, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # save our attributes\n",
    "        self.input_size = input_size\n",
    "        self.num_layers=num_layers\n",
    "        self.width=width\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        # Define the layers of our neural net\n",
    "        layers = [nn.Linear(input_size, width)]\n",
    "        layers += [\n",
    "            nn.Sequential(\n",
    "                nn.?,\n",
    "                nn.?,\n",
    "                nn.?\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        layers.append(nn.Linear(width, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*layer`s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = MyNeuralNet(input_size=13, num_layers=5)\n",
    "my_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Same training loop from [last workshop](https://github.com/sjsumlclub/workshop-anatomy-pytorch-project/blob/master/Anatomy%20of%20a%20PyTorch%20Project%20WORKSHOP.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert training loop here\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_net = my_net.to(device)\n",
    "\n",
    "# import torch.optim\n",
    "import torch.optim as optim\n",
    "# instantiate an optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# train for n epochs. an epoch is a full iteration through our dataset\n",
    "num_epochs = 10\n",
    "\n",
    "# create something to track of accuracy over time\n",
    "accuracies = []\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "    \n",
    "    # track our accuracy\n",
    "    correct_this_epoch = 0.\n",
    "    \n",
    "    # loop over our data loader\n",
    "    for data, labels in tqdm(my_dataloader, desc=\"Batch\", leave=False):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # pass data through model\n",
    "        outputs = my_net(data)\n",
    "        # calculate the loss\n",
    "#         print(outputs.size())\n",
    "#         print(labels.long().size())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        # Use our optimizer to update the network\n",
    "        # 1: zero_grad our optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # 2: run a backward pass\n",
    "        loss.backward()\n",
    "        # 3: make a step\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_this_epoch += torch.sum(preds==labels.data.long())\n",
    "        \n",
    "    accuracy_this_epoch = correct_this_epoch.double() / len(my_dataset)\n",
    "    print(accuracy_this_epoch)\n",
    "    accuracies.append(accuracy_this_epoch.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
